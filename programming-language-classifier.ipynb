{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Language Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Glob to read each type of file in the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The text of each file is loaded into a list, and the type of file is loaded into another list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_code(directory, ftype):\n",
    "    files = glob.glob('{}/**/*.{}'.format(directory, ftype), recursive=True)\n",
    "    for fiyl in files:\n",
    "        y_file_type.append(ftype)\n",
    "        with open(fiyl) as f:\n",
    "            x_texts.append(f.read())\n",
    "\n",
    "\n",
    "x_texts = []\n",
    "y_file_type = []\n",
    "\n",
    "\n",
    "file_types = ['gcc', 'c', 'csharp', 'sbcl', 'clojure', 'java', 'javascript', \n",
    "              'ocaml', 'perl', 'hack', 'php', 'python3', 'jruby', 'yarv', \n",
    "              'scala', 'racket', 'ghc']\n",
    "\n",
    "for ft in file_types:\n",
    "    read_code('/Users/David/documents/tiy/programming-language-classifier/benchmarksgame-2014-08-31/benchmarksgame/bench/', ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The first part of the pipeline is the CountVectorizer.  The CountVectorizer works by turning 'words' of at least two characters into a numeric representation that can be passed into a classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part of the pipeline is the TfiftTransformer.  This takes the vectorized data and weighs each feature based on how unique it is.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final part of the pipeline is the estimator method, which in this case is LinearSVC.  LinearSVC works by creating a vector based on the training data features.  When using predict, LinearSVC will decide if the feature is close enough to that vector to be considered the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_map = [\n",
    "                ('hashin', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('lin', svm.LinearSVC()),\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(pipeline_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into two parts, training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_texts, y_file_type, test_size=0.4, random_state=1965)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing the training data into the pipeline and printing out the score for our training data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .96 is a pretty good $R^{2}$.  That means that our model can predict the correct programming langues 96% of the time, when using the data that we trained it with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.968571428571\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(pipeline.score(X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that the pipeline is trained, we pass the test data in to see how well our model can predict data it has seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82905982906\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An $R^{2}$ score of .829 is not too bad.  This means that the model will predict the correct language 82.9% when the model is fed a new set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how well the model does with Bryce's testing files which are about 3 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is a list of all of the correct programming languages that correspond to the test files that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bryce_tests = ['clojure', 'clojure', 'clojure', 'clojure','python3', 'python3',\n",
    "               'python3',  'python3',  'javascript', 'javascript', 'javascript',\n",
    "               'javascript', 'jruby', 'jruby', 'jruby', 'ghc', 'ghc',\n",
    "                'ghc', 'racket', 'racket', 'racket', 'java', 'java',\n",
    "                'scala', 'scala', 'php', 'php', 'php', 'ocaml',\n",
    "                'ocaml']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The for loop below is running through each file in the directory and checking the models prediction against the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "count = 0\n",
    "num_correct = 0\n",
    "for n in list(range(1,25)) + list(range(28,32)):\n",
    "    type_count = {}\n",
    "    with open('test/{}'.format(n)) as f:\n",
    "        f = f.read()\n",
    "        predicter = pipeline.predict([f])\n",
    "        predictions.append(predicter)\n",
    "        if predicter == bryce_tests[count]:\n",
    "            num_correct += 1\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying the percentage the model was able to predict correctly.  76.7% is not too bad, but not quite as good as the model did with the original test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(num_correct/len(bryce_tests))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More model testing using 75 MB of test files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A peer compiled a directory of several examples of each programming language.  Again, we are comparing the model's prediction to the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_texts = []\n",
    "y_file_type = []\n",
    "\n",
    "\n",
    "file_types = ['gcc', 'c', 'csharp', 'sbcl', 'clojure', 'java', \n",
    "              'javascript', 'ocam', 'perl', 'hack', 'php', 'python3',\n",
    "              'jruby', 'yarv', 'scala', 'racket', 'ghc', 'cs']\n",
    "\n",
    "\n",
    "for ft in file_types:\n",
    "    try:\n",
    "        read_code('/Users/David/documents/tiy/programming-language-classifier/data/', ft)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "file_dict = dict(zip(x_texts, y_file_type))\n",
    "\n",
    "correct_count = 0\n",
    "for key, value in file_dict.items():\n",
    "    predicter = pipeline.predict([key])\n",
    "    if predicter == value:\n",
    "        correct_count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying the percentage the model was able to predict correctly.  71.8% correct is a little worse than previous results, but still not too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7187039764359352\n"
     ]
    }
   ],
   "source": [
    "print(correct_count/len(file_dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
